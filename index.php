<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/1999/REC-html401-19991224/loose.dtd"> 
<html> 
  <head> 
    <title> 
    robert pless - george washington university - computer vision 
    </title> 
    <meta name="keywords" content="robert pless, computer vision, webcams, generalized cameras structure from motion""> 
     <link rel="STYLESHEET" href="./mystyle.css" type="text/css">   
  </head> 
  <body> 
<?php include("navbar.html") ?>
<div class="content"> 
<h4>Updates:</h4>
<ul>
<li>I'm excited to be part of the brand new UMD + GW + Morgan State NSF Trails Institute, <a href="https://www.trails.umd.edu/research">Trustworthy AI in Law and Society</a>.                                                                                                   
<li>What we learned from hundreds of varieties of Sorghum planted in an amazing field site in Arizona, and imaging them with 3D scanners, thermal cameras and more.
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9289439/">Comparing Deep Learning Approaches for Understanding Genotype Ã— Phenotype Interactions in Biomass Sorghum< [Frontiers in AI]/a>
<li>"GradCAM" style visualization tools for image similarity metrics learned by transformers <a href="https://openaccess.thecvf.com/content/WACV2022/html/Black_Visualizing_Paired_Image_Similarity_in_Transformer_Networks_WACV_2022_paper.html">"Visualizing Paired Image Similarity in Transformer Networks
 [WACV 2022]</a>.
<li><a href="https://arxiv.org/abs/2007.12749">Hard Negatives are Hard, But Useful</a>.  An ECCV 2020 paper showing how to make metric learning with triplet loss effective with hard-negative examples.
</ul>

<h4>Research</h4> 
My research is in the area of computer vision with applications to
environmental science, medical imaging, robotics and virtual reality.
I am particularly interested in data-driven and geometric techniques
to more robustly understand images taken "in the wild".  This research
exploits the fact that cameras are incredibly precise measurement
systems --- if they are calibrated properly, then the vast quantities
of visual data they collect can help to learn, understand, and
manipulate the world around us.  At a high level, the current themes
of research in my lab are:

<ul>
<li><b>Understand visual change at scales from the sidewalk to the planet:</b><br>
What can you do with a billion images?  Webcams, iPhones, and flocks
  of micro-satellites a visual depiction of the Earth at an
unprecedented temporal and spatial coverage.  Our aim is to organize
these disparate image sources to create coherent global imaging
systems that answer important questions facing our society and our
planet.  Specifically, we work to understand physical principles that
govern image formation in realistic environments, and we use those in
application domains that include: Understanding patterns of
tree-growth at a continental scale, characterizing the use of public
spaces over time, and creating generalizable models to 
learn how image appearance varies over time.

<li><b>Next generation imaging systems:</b><br>

All Virtual Reality and Robotics applications require fast visual
reasoning systems to characterize the 3D world and pose of objects
within that world.  The objective of this research effort is to
understand and overcome the fundamental limits of cameras and
materials that make that task difficult.  Specifically, our work has
developed the theory of motion estimation from multi-perspective and
compressive sensing cameras, fundamental constraints for calibrating
cameras with other sensing systems, and the design of new light-field
modulating materials that make geometric inference especially easy.  

<li><b>Democratizing Visual Analytics and Applications to Social Justice:</b><br>
What can our community do to make visual reasoning more available to
more people?  Our lab works to make web implementations and
smart-phone apps that support the broader publics ability to create
and use visual inference across many applications.  This includes apps
to support Citizen Science through repeat photography, and the
geocalibration.org website that allows the precise geo-location of an
image with respect to Google Maps which was used to find the lost
grave of a Jane Doe crime victim from 30 years ago.
</ul>

<br>

<h4>Dataset, Apps, and Frequenty Requested Code</h4>
<ul>
<li> The Archive of Many Outdoor Scenes 
(<a href="http://amos.cse.wustl.edu">AMOS</a>).
<li><a href="http://traffickcam.org">TraffickCam</a>, an app to crowd source the creation of a continually
  updated index of the appearance of hotel rooms to support
  investigations of sex trafficking.
<li> Citizen Science Repeat Photography App and Data 
(<a href="http://projectrephoto.com">rePhoto</a>).
<li>Code (<a href="code/manifoldVis.zip">12 KB zip file</a>) for
matlab and web visualizations of image manifolds, from the paper: "A
Survey of Manifold Learning for Images", Robert Pless and Richard
Souvenir, IPSJ Transactions on Computer Vision and Applications,
vol. 1 pp. 83-94, 2009.
<a href="http://www2.seas.gwu.edu/~pless/pubs/bibtexbrowser.php?key=plessSouvenirSurvey2009&bib=plessRefs.bib">(bib)</a>

<li>Code (<a href="code/calibZip.zip">2.3 MB zip file</a>) from our
paper: "Extrinsic Calibration of a Camera and Laser Range Finder",
Qilong Zhang, Robert Pless, IROS 2004,
<a href="http://www2.seas.gwu.edu/~pless/pubs/bibtexbrowser.php?key=zhangIROS&bib=plessRefs.bib">(bib)</a>, 
</ul>
</div> 
</body> 
</html> 
 
